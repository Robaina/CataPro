{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa6bc28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Starting model download process...\n",
      "Preparing to download 'Rostlab/prot_t5_xl_uniref50' into 'models/prot_t5_xl_uniref50'...\n",
      "/home/ubuntu/miniconda/envs/proteus/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "spiece.model: 100%|███████████████████████████| 238k/238k [00:00<00:00, 162MB/s]\n",
      "special_tokens_map.json: 1.79kB [00:00, 12.2MB/s]\n",
      "tokenizer_config.json: 100%|██████████████████| 24.0/24.0 [00:00<00:00, 237kB/s]\n",
      "config.json: 100%|█████████████████████████████| 546/546 [00:00<00:00, 5.77MB/s]\n",
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "pytorch_model.bin: 100%|████████████████████| 11.3G/11.3G [00:43<00:00, 258MB/s]\n",
      "Successfully downloaded and saved 'Rostlab/prot_t5_xl_uniref50'.\n",
      "------------------------------\n",
      "Preparing to download 'laituan245/molt5-base-smiles2caption' into 'models/molt5-base-smiles2caption'...\n",
      "spiece.model: 100%|███████████████████████████| 792k/792k [00:00<00:00, 130MB/s]\n",
      "special_tokens_map.json: 1.79kB [00:00, 10.9MB/s]\n",
      "tokenizer_config.json: 2.13kB [00:00, 12.7MB/s]\n",
      "config.json: 100%|█████████████████████████████| 699/699 [00:00<00:00, 6.72MB/s]\n",
      "pytorch_model.bin: 100%|█████████████████████| 990M/990M [00:09<00:00, 99.1MB/s]\n",
      "Successfully downloaded and saved 'laituan245/molt5-base-smiles2caption'.\n",
      "------------------------------\n",
      "\n",
      "Model download complete!\n",
      "Your models are now available in the 'models' directory.\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!python download_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37ebc9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/app/inference/predict.py\", line 1, in <module>\n",
      "    import torch as th\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 1382, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/functional.py\", line 7, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/app/inference/predict.py\", line 86, in <module>\n",
      "    ezy_ids, smiles_list, dataloader = get_datasets(inp_fpath, ProtT5_model, MolT5_model)\n",
      "  File \"/app/inference/predict.py\", line 32, in get_datasets\n",
      "    seq_ProtT5 = Seq_to_vec(sequences, ProtT5_model)\n",
      "  File \"/app/inference/utils.py\", line 23, in Seq_to_vec\n",
      "    model = T5EncoderModel.from_pretrained(ProtT5_model)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 311, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 4839, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 5302, in _load_pretrained_model\n",
      "    _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 920, in load_shard_file\n",
      "    state_dict = load_state_dict(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 556, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1517, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\ndocker run --rm --gpus all \\\\\\n  -v /home/ubuntu/lab4/CataPro/samples:/app/samples \\\\\\n  -v /home/ubuntu/lab4/CataPro/models:/app/models \\\\\\n  protscout-tools-catapro \\\\\\n  -inp_fpath /app/samples/sample_inp.csv \\\\\\n  -model_dpath /app/models \\\\\\n  -batch_size 64 \\\\\\n  -device cuda:0 \\\\\\n  -out_fpath samples/catapro_prediction.csv\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdocker run --rm --gpus all \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  -v /home/ubuntu/lab4/CataPro/samples:/app/samples \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  -v /home/ubuntu/lab4/CataPro/models:/app/models \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  protscout-tools-catapro \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  -inp_fpath /app/samples/sample_inp.csv \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  -model_dpath /app/models \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  -batch_size 64 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  -device cuda:0 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  -out_fpath samples/catapro_prediction.csv\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2543\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2542\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2543\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/magics/script.py:159\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/magics/script.py:336\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    335\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\ndocker run --rm --gpus all \\\\\\n  -v /home/ubuntu/lab4/CataPro/samples:/app/samples \\\\\\n  -v /home/ubuntu/lab4/CataPro/models:/app/models \\\\\\n  protscout-tools-catapro \\\\\\n  -inp_fpath /app/samples/sample_inp.csv \\\\\\n  -model_dpath /app/models \\\\\\n  -batch_size 64 \\\\\\n  -device cuda:0 \\\\\\n  -out_fpath samples/catapro_prediction.csv\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "docker run --rm --gpus all \\\n",
    "  -v /home/ubuntu/lab4/CataPro/samples:/app/samples \\\n",
    "  -v /home/ubuntu/lab4/CataPro/models:/app/models \\\n",
    "  protscout-tools-catapro \\\n",
    "  -inp_fpath /app/samples/sample_inp.csv \\\n",
    "  -model_dpath /app/models \\\n",
    "  -batch_size 64 \\\n",
    "  -device cuda:0 \\\n",
    "  -out_fpath samples/catapro_prediction.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proteus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
